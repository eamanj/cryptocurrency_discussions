#!/usr/bin/python

import argparse
import os
import sys
import csv
import pandas as pd
import numpy as np
import utils
import mutual_info as mi
import scipy.stats
from datetime import timedelta, datetime
from math import log
from collections import defaultdict

parser = argparse.ArgumentParser(
    description="This script extracts stats per each subject thread on the forum. "
                "The stats can be computed over two periods: 1-Initial: right before "
                "each coin is introduced to the market 2- Fixed: The initial period "
                "will be different for each coin. In the fixed period, stats for all "
                "coins will be computed over the same period whose end date will be "
                "provided as command line.")
parser.add_argument("forum_input",
                    help="The location of CSV file containing posts from all forums "
                    "sorted by date and time. The last two columns of file should "
                    "contain the list of modified and unmodified coins mentions in the "
                    "post.")
parser.add_argument("coins_info",
                    help="A CSV file containing the earliest introduction and trade "
                    "dates for each coin. It also has the urls of different announcement "
                    "threads. The first line is header with info like: "
                    "symbol,name,earliest_trade_date,,user1,user1_url,user1_date"
                    "user2,user2_url,user2_date... This file is generated by "
                    "extract_coin_users.py.")
parser.add_argument("output_dir",
                    help="The directory where the subjects stats will be written to. "
                    "There will be two files, one containing all subjects with "
                    "subject_id in the first column and its url in the second column. "
                    "The second output will contain coin, subject_id, url, "
                    "introduction_date and trade date as first few columns.")
parser.add_argument("-e", dest="end_date", default=None,
                    help="If provided, it will be the end date (YYYY-MM-DD) of the "
                    "period during which we will compute thread metrics. If not "
                    "provided, the default would use the earliest trade date as the end "
                    "date. As opposed to default behavior, when end_date is specified "
                    "this period will be the same for all coins and the date should be "
                    "specified in YYYY-MM-DD format. The length of the period, which "
                    "will determine the start date can be  "
                    "specified seperately")
parser.add_argument("-l", dest="period_length", type=int, default=600,
                    help="The length of the period (days) during which we will compute "
                    "thread metrics. Depending on whether end_date is specified, the "
                    "period will end in end_date or earliest_trade_date. This argument "
                    "will determine the length of period, or effectively its start date. "
                    "Default is 600 days.")
parser.add_argument("-tq", dest="num_posts_quantile_threshold", type=float, default=0,
                    help="Defines the threshold for the minimum number of posts a user "
                    "should have made in a thread before it's considered the user has "
                    "actually contributed to the thread. This is expressed in terms of "
                    "quantiles in the distribution of num posts per each user in the "
                    "thread. So it gets translated to different threshold in terms of "
                    "absolute number of posts per each thread since each thread has a "
                    "different distribution of num posts per user. Default is 0. and "
                    "it should be a number between 0 and 1.")
parser.add_argument("-ta", dest="num_posts_absolute_threshold", type=int, default=0,
                    help="The threshold for the absolute minimum number of posts a user "
                    "should have made in a thread before it's considered the user has "
                    "actually contributed to the thread. In contrast to quantile "
                    "thresholding, this is in terms of absolute number of posts. Default "
                    "is 0. Should be an integer.")
parser.add_argument("-s", dest="earliest_trade_date_shift", type=int, default=0,
                    help="The number of days to shift the earliest trade date to the "
                    "future. The first few days of trading might not have any volume, "
                    "be noisy or simply wrong. So we can shift the earliest trade date "
                    "a bit into the future, which effectively includes more posts up "
                    "to a later date for each announcement thread. Default is 0, which "
                    "means no shift.")
parser.add_argument("-v", dest="valid_coin_symbols",
                    help="If provided, it has to be a file with the symbol of valid "
                    "coins, one per line. If provided, the metrics only for these "
                    "coins will be computed. Also all the metrics that relate the coin "
                    "to other announcement threads will be measured based on the coins "
                    "present in this set and not all the coins present in coins_info "
                    "For example, num_unique_announcement_threads will contain the "
                    "number announcement threads belonging to these limited coins that "
                    "are connected to the coin of interest.")
args = parser.parse_args()


# quantiles to compute for the number of posts made by each user in the thread
POSTS_QUANTILES = [0.01, 0.02, 0.05, 0.75, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5]


def compute_continous_stats(data):
  ''' Stats for a continous random variables. we treat this as a sample drawn from
  continuous range '''
  result = dict()
  result['mean'] = 0.0
  result['median'] = 0.0
  result['std'] = 0.0
  result['entropy'] = 0.0
  result['max'] = 0.0
  result['min'] = 0.0
  result['discrete_entropy'] = 0.0
  if len(data) > 0:
    result['mean'] = np.mean(data)
    result['median'] = np.median(data)
    result['std'] = np.std(data)
    result['max'] = np.max(data)
    result['min'] = np.min(data)
   
    # discretize data and compute discrete entropy. First, normalize data between 0 and 1
    if np.max(data) != np.min(data):
      num_bins = 20.0
      norm_data = list((data - np.min(data)) / (np.max(data) - np.min(data)))
      bin_width = 1.0/num_bins
      bins = np.arange(0, 1.00001, bin_width)
      discretized_data = bins[np.digitize(norm_data, bins, right=True)]
      discretized_data = pd.Series(pd.Categorical(discretized_data, categories=bins))
      discretized_counts = discretized_data.groupby(discretized_data).count()
      prob = [float(x)/sum(discretized_counts) for x in discretized_counts]
      result['discrete_entropy'] = scipy.stats.entropy(prob)
    

    if len(data) > 1:
      d = np.array(data, dtype=float, ndmin=2).T
      num_neighbors = min(3, len(data) - 1)
      result['entropy'] = mi.entropy(d, num_neighbors)
  return result


def compute_discrete_stats(data):
  ''' Stats for a discrete random variables '''
  result = dict()
  num_unique = len(data)
  if num_unique:
    mean= np.mean(data)
    prob = [float(x)/sum(data) for x in data]
    entropy = scipy.stats.entropy(prob)
    if num_unique > 1:
      entropy_normalized = entropy / log(1.0*num_unique)
    else:
      entropy_normalized = 0
  else:
    mean = 0
    entropy = 0
    entropy_normalized = 0
  
  result = dict()
  result['num_unique'] = num_unique
  result['mean'] = mean
  result['entropy'] = entropy
  result['entropy_normalized'] = entropy_normalized
  return result
  

def get_conversation_branches(announcement_posts):
  ''' Given the announcement pots sorted in chronological order as a dataframe, it goes
  through them one by one and constructs a list that contains the length of converstaion
  branches. Each conversation corresponds to a reply brach in the thread (if you think of
  replies as a tree).
  Returns a list that contains the length of each reply branch in the thread converstaion
  tree.
  '''
  # This assumes the data is sorted in time and pandas returns the rows in the same
  # chronological order
  conversation_branches = list()
  for post_id, row in announcement_posts.iterrows():
    num_quotes = row['num_quotes']
    if num_quotes == 0:
      continue
    # go through the conversation branches and find the first occurence of a branch that
    # has (num_quotes - 1) replies. If you find one, the current num_quotes corresponds to
    # the continutation of the same branch (another reply in the same branch), so update
    # the branch length in place. If you don't find a branch that has (num_quotes - 1)
    # length, then the new post must have started a new branch: so append it to the branch
    # list.
    idx = 0
    for x in conversation_branches:
      if x == (num_quotes - 1):
        conversation_branches[idx] = num_quotes
        break
      idx += 1
    # if reached the end of branches, this post must have started a new branch
    if idx == len(conversation_branches):
      conversation_branches.append(num_quotes)
  return conversation_branches
  

def compute_polarization_stats(data, min_val, max_val):
  ''' Normalizes the data to 0 to 1 range using the min and max provided. It assumes a
  linear scale from min to max. It returns a set of statistics that capture the
  polarization or the distribution of data. For example, it measure the mean,
  kurtosis and normalized entropy of the normalized data. It also computes the Morales
  polarization score using the distribution of negative (<0.5) and positive (>0.5)
  scores. This function is mainly written for sentiment scores in a fixed linear range'''
  mean = 0
  skewness = 0
  kurtosis = 0
  entropy_normalized = 0
  pairwise_diffs_mean = 0
  gravity_centers_distance = 0
  negative_positive_size_diff = 0
  morales_polarization = 0
  if len(data) > 0:
    num_bins = 20.0
    # normalize data between 0 and 1
    norm_data = list((data - min_val) / (max_val - min_val))
    bins = np.arange(0, 1.0000001, 1.0/num_bins)
    discretized_data = bins[np.digitize(norm_data, bins, right=True)]
    discretized_data = pd.Series(pd.Categorical(discretized_data, categories=bins))
    discretized_counts = discretized_data.groupby(discretized_data).count()
    discretized_counts_stats = compute_discrete_stats(discretized_counts)

    # mean/skewness/kurtosis of normalized data and entropy of discretized version of
    # normalized data
    mean = np.mean(norm_data)
    skewness = scipy.stats.skew(norm_data)
    kurtosis = scipy.stats.kurtosis(norm_data)
    entropy_normalized = discretized_counts_stats['entropy_normalized']

    # average pairwise absolute difference
    pairwise_diffs_sum = 0
    for i in range(len(norm_data)):
      for j in range(i+1, len(norm_data)):
        pairwise_diffs_sum += abs(norm_data[i] - norm_data[j])
    num_pairs = (len(norm_data) * (len(norm_data) - 1)) / 2
    pairwise_diffs_mean = (pairwise_diffs_sum / num_pairs) if num_pairs else 0

    # polarization as defined by Morales. 0.5 counts as neutral. Below 0.5 is negative,
    # above 0.5 is positive.
    # We remove neutral data and compute morales polarization on portion of data that is not
    # neutral. We have to do this because a big portion of the data is neutral. Another
    # possibility is to half of neutral to positive, and the other half to negative
    negative_norm_data = [x for x in norm_data if x < 0.5]
    positive_norm_data = [x for x in norm_data if x > 0.5]
    #negative_norm_data.extend(neutral_norm_data[0:(neutral_size/2)])
    #positive_norm_data.extend(neutral_norm_data[(neutral_size/2):])
    negative_positive_size = len(negative_norm_data) + len(positive_norm_data)
    if negative_positive_size:
      negative_probability = len(negative_norm_data) * 1.0 / negative_positive_size
      positive_probability = len(positive_norm_data) * 1.0 / negative_positive_size
    else:
      negative_probability = np.nan
      positive_probability = np.nan

    negative_gravity_center = np.mean(negative_norm_data)
    positive_gravity_center = np.mean(positive_norm_data)
    gravity_centers_distance = abs(positive_gravity_center - negative_gravity_center)
    negative_positive_size_diff = abs(positive_probability - negative_probability)
    morales_polarization = (1 - negative_positive_size_diff) * gravity_centers_distance

  result = {'mean': mean, 'skewness': skewness, 'kurtosis': kurtosis,
            'entropy_normalized': entropy_normalized,
            'pairwise_diffs_mean': pairwise_diffs_mean,
            'gravity_centers_distance': gravity_centers_distance,
            'negative_positive_size_diff': negative_positive_size_diff,
            'morales_polarization': morales_polarization}
  return result
  
def generate_network(announcement_thread_ids_by_symbol, posts, coin_data_by_symbol):
  ''' extract the connections TO this coin from other announcement threads weighted. This
  will be a directed graph of linkages among coins. There will be link from coin A to coin
  B, if users that have already posted in coin A thread at an earlier time, are now
  posting in coin B thread. Note that here, we are only extracting the edges to coin for
  which the method is called on. We should call this method separately for other coins to
  get their edges. Weights measure similarity and are determined by:
  1- number of announcement users who have also made posts in the other coin at an earlier
  time.
  2- jaccard coefficient of users: number of common announcement users over union of
  announcement users in both
  3- fraction of the posts made by common announcement user in this coin thread multiplied
  by the fraction of the posts made by same users in other coins thread.
  '''
  # get number of posts made by each user in each coin thread, before end date and between
  # start and end dates (lim version)
  symbols_by_thread_id = {thread_id : symbol
                          for symbol, thread_ids in announcement_thread_ids_by_symbol.items()
                          for thread_id in thread_ids}
  all_announcement_thread_ids = [id for ids in announcement_thread_ids_by_symbol.values() for id in ids]
  all_announcement_posts = posts.loc[posts['thread_id'].isin(all_announcement_thread_ids)]
  all_announcement_posts['symbol'] = all_announcement_posts['thread_id'].map(symbols_by_thread_id)

  # add columns for posts end-date and start-dates that might be different for each coin
  # and then select posts within the date range
  if args.end_date:
    all_announcement_posts['end_date'] = pd.to_datetime(args.end_date)
    all_announcement_posts['start_date'] = (all_announcement_posts['end_date'] -
                                            pd.to_timedelta(args.period_length, unit='d'))
  else:
    # earliest trade date as end date
    earliest_trade_dates = coin_data_by_symbol['earliest_trade_date'].to_dict()
    all_announcement_posts['end_date'] = all_announcement_posts['symbol'].map(earliest_trade_dates)
    all_announcement_posts['end_date'] = pd.to_datetime(all_announcement_posts['end_date'])
    all_announcement_posts['end_date'] = (all_announcement_posts['end_date'] +
                                          pd.to_timedelta(args.earliest_trade_date_shift, unit='d'))
    all_announcement_posts['start_date'] = (all_announcement_posts['end_date'] -
                                            pd.to_timedelta(args.period_length, unit='d'))

  # filter posts to be within the respective date range of each coin
  all_announcement_posts = all_announcement_posts.loc[all_announcement_posts['date'] <=
                                                      all_announcement_posts['end_date']]
  all_announcement_posts_lim = all_announcement_posts.loc[all_announcement_posts['date'] >=
                                                          all_announcement_posts['start_date']]
 
  all_announcement_posts = all_announcement_posts.groupby(['symbol', 'user']).size()
  all_announcement_posts_lim = all_announcement_posts_lim.groupby(['symbol', 'user']).size()
  all_announcement_posts = all_announcement_posts[
      all_announcement_posts >= args.num_posts_absolute_threshold]
  all_announcement_posts_lim = all_announcement_posts_lim[
      all_announcement_posts_lim >= args.num_posts_absolute_threshold]

  # now for each announcement thread, remove posts made by users who contributed less than
  # the quantile threshold. for this, we need to compute distribution of number of posts
  # per user in each symbol thread and remove contributions to each thread that are below
  # the threshold per each thread
  if args.num_posts_quantile_threshold > 0:
    all_announcement_posts = all_announcement_posts.reset_index('user', name='num_posts')
    all_announcement_posts_lim = all_announcement_posts_lim.reset_index('user', name='num_posts')
    # reduce to those users that are above quantile, only if the all_announcement_posts is
    # not empty (otherwise it will give an error since the result becomes an empty data
    # frame without any columns)
    if not all_announcement_posts.empty:
      all_announcement_posts = all_announcement_posts.groupby('symbol').apply(
          lambda x: x[x['num_posts'] >=
                      x['num_posts'].quantile(args.num_posts_quantile_threshold,
                                              interpolation='linear')])
    all_announcement_posts = all_announcement_posts.reset_index(1, drop=True)
    all_announcement_posts = all_announcement_posts.set_index('user', append=True)
    all_announcement_posts = all_announcement_posts['num_posts']
    
    if not all_announcement_posts_lim.empty:
      all_announcement_posts_lim = all_announcement_posts_lim.groupby('symbol').apply(
          lambda x: x[x['num_posts'] >=
                      x['num_posts'].quantile(args.num_posts_quantile_threshold,
                                              interpolation='linear')])
    all_announcement_posts_lim = all_announcement_posts_lim.reset_index(1, drop=True)
    all_announcement_posts_lim = all_announcement_posts_lim.set_index('user', append=True)
    all_announcement_posts_lim = all_announcement_posts_lim['num_posts']

  all_announcement_posts = all_announcement_posts.sort_index()
  all_announcement_posts_lim = all_announcement_posts_lim.sort_index()
  
  # generated the undirected network in which only concurrent co-posts are counted
  symbols = all_announcement_posts_lim.index.get_level_values('symbol').unique()
  rows = list()
  for i in range(len(symbols)):
    symbol1 = symbols[i]
    symbol1_users = all_announcement_posts_lim.loc[symbol1].index.values
    symbol1_total_posts = all_announcement_posts_lim.loc[symbol1].sum()
    if symbol1_total_posts == 0:
      continue

    for j in range(i+1, len(symbols)):
      symbol2 = symbols[j]
      symbol2_users = all_announcement_posts_lim.loc[symbol2].index.values 
      symbol2_total_posts = all_announcement_posts_lim.loc[symbol2].sum()
      if symbol2_total_posts == 0:
        continue
      
      users_intersect = np.intersect1d(symbol1_users, symbol2_users)
      users_union = np.union1d(symbol1_users, symbol2_users)
      if users_intersect.size == 0:
        continue

      symbol1_intersect_posts = all_announcement_posts_lim.loc[symbol1].loc[users_intersect].sum()
      symbol2_intersect_posts = all_announcement_posts_lim.loc[symbol2].loc[users_intersect].sum()
      symbol1_intersect_posts_fraction = symbol1_intersect_posts * 1.0 / symbol1_total_posts
      symbol2_intersect_posts_fraction = symbol2_intersect_posts * 1.0 / symbol2_total_posts
      row = (symbol1, symbol2,
             len(symbol1_users), len(symbol2_users),
             len(users_intersect), len(users_union),
             len(users_intersect) * 1.0 / len(users_union),
             symbol1_total_posts, symbol2_total_posts,
             symbol1_intersect_posts, symbol2_intersect_posts,
             symbol1_intersect_posts_fraction, symbol2_intersect_posts_fraction,
             symbol1_intersect_posts_fraction * symbol2_intersect_posts_fraction)
      rows.append(row)
  
  columns = ['symbol1', 'symbol2', 'symbol1_users', 'symbol2_users',
             'users_intersect', 'users_union',
             'jaccard_users_intersect', 'symbol1_total_posts', 'symbol2_total_posts',
             'symbol1_intersect_posts', 'symbol2_intersect_posts',
             'symbol1_intersect_posts_fraction', 'symbol2_intersect_posts_fraction',
             'intersect_posts_commonality']
  undirected_edges = pd.DataFrame.from_records(rows, columns = columns)
  
  # generated the directed network in which the whole posting history of the source node
  # is included, whereas only the limited time posting history is considered for the
  # destination node. The idea is to capture the flow of information from earlier coins to
  # later ones
  symbols = all_announcement_posts.index.get_level_values('symbol').unique()
  symbols_lim = all_announcement_posts_lim.index.get_level_values('symbol').unique()
  rows = list()
  for symbol1 in symbols:
    # use the full history of source node (symbol1)
    symbol1_users = all_announcement_posts.loc[symbol1].index.values
    symbol1_total_posts = all_announcement_posts.loc[symbol1].sum()
    if symbol1_total_posts == 0:
      continue
    
    for symbol2 in symbols_lim:
      # use the limited history for the destination node (symbol2)
      symbol2_users = all_announcement_posts_lim.loc[symbol2].index.values 
      symbol2_total_posts = all_announcement_posts_lim.loc[symbol2].sum()
      if symbol2_total_posts == 0:
        continue
      
      users_intersect = np.intersect1d(symbol1_users, symbol2_users)
      users_union = np.union1d(symbol1_users, symbol2_users)
      if users_intersect.size == 0:
        continue

      symbol1_intersect_posts = all_announcement_posts.loc[symbol1].loc[users_intersect].sum()
      symbol2_intersect_posts = all_announcement_posts_lim.loc[symbol2].loc[users_intersect].sum()
      symbol1_intersect_posts_fraction = symbol1_intersect_posts * 1.0 / symbol1_total_posts
      symbol2_intersect_posts_fraction = symbol2_intersect_posts * 1.0 / symbol2_total_posts
      row = (symbol1, symbol2,
             len(symbol1_users), len(symbol2_users),
             len(users_intersect), len(users_union),
             len(users_intersect) * 1.0 / len(users_union),
             symbol1_total_posts, symbol2_total_posts,
             symbol1_intersect_posts, symbol2_intersect_posts,
             symbol1_intersect_posts_fraction, symbol2_intersect_posts_fraction,
             symbol1_intersect_posts_fraction * symbol2_intersect_posts_fraction)
      rows.append(row)
  
  columns = ['source', 'destination', 'source_users', 'destination_users',
             'users_intersect', 'users_union',
             'jaccard_users_intersect', 'symbol1_total_posts', 'symbol2_total_posts',
             'symbol1_intersect_posts', 'symbol2_intersect_posts',
             'symbol1_intersect_posts_fraction', 'symbol2_intersect_posts_fraction',
             'intersect_posts_commonality']
  directed_edges = pd.DataFrame.from_records(rows, columns = columns)
  return (undirected_edges, directed_edges)


def compute_coin_coposts_stats_by_period(announcement_thread_ids_by_symbol, posts,
                                         announcement_users, start_date, end_date,
                                         output):
  ''' Computes the stats of the coin announcement thread/users derived from their
  co-posting behavior and the network properties. It also generates the edges of this coin
  in the co-post network.

  posts: all posts made by all users in all threads.
  announcement_users: the users who have made a post in the announcement thread between
  start and end dates.
  '''
  # concurrent: a boolean value that determines how stats for co-posts on other threads is
  # computed. Recall that co-posts in other threads determine the strength of connection
  # to other threads, possible sources of information. If concurrent is True then co-posts
  # in other threads (announcment or not) that only happen between start_date and end_datr
  # are counted. In other words, co-posts are counted only in a period concurrent with the
  # posts that we are counting in the announcement thread of the coin itself. If
  # concurrent is False, we count the posts in other threads (by announcement users) at
  # any date before end-date potentially even before start-date. The idea is to capture
  # historical strength to other threads, not just during start-end date period.
  #
  # Note that the analysis of the posts made in the actual announcement post of the coin
  # itslef will be limited to start-date and end-date. The concurrency only affects
  # counting of co-posts by announcement users in threads other than the announcement
  # thread.  So co-post period is start-date:end-date if concurrent is true and :end-date
  # if concurrent if false.
  
  for concurrent in [True, False]:
    # should we limit to start-end dates or anything before end date?
    if(concurrent):
      user_all_posts = posts.loc[(posts['user'].isin(announcement_users)) &
                                 (posts['date'] <= end_date) &
                                 (posts['date'] >= start_date)]
    else:
      user_all_posts = posts.loc[(posts['user'].isin(announcement_users)) &
                                 (posts['date'] <= end_date)]

    user_all_posts_count = user_all_posts.groupby('user').size()
    user_all_posts_stats = compute_discrete_stats(user_all_posts_count)

    # extract the stats on the posts made by announcement users in the main bitcoin forum.
    # get the total number of posts in the main bitcoin board, number of such users and
    # the average number of posts made by each.
    BITCOIN_FORUM_ID = 1
    bitcoin_forum_user_posts = user_all_posts.loc[user_all_posts['forum_id'] == BITCOIN_FORUM_ID]
    bitcoin_forum_user_posts_count = bitcoin_forum_user_posts.groupby('user').size()
    bitcoin_forum_user_posts_stats = compute_discrete_stats(bitcoin_forum_user_posts_count)
    bitcoin_forum_total_num_posts = len(bitcoin_forum_user_posts.index)
    
    # user stats in terms of participation in all threads, but not just the number of posts,
    # instead number of threads.  Need number of threads the users have participated during
    # the co-post period. But first get the unique threads each user has participated in
    # from posts
    #user_all_threads = user_all_posts[['user', 'thread_id']].drop_duplicates() 
    user_all_threads = user_all_posts.groupby(['user', 'thread_id']).size()
    user_all_threads = user_all_threads[user_all_threads >= args.num_posts_absolute_threshold]
  
    # need to compute distribution of number of posts per user in each thread and remove
    # contributions to each thread that are below the threshold per each thread
    user_all_threads = user_all_threads.reset_index('user', name='num_posts')
    # reduce to those users that are above quantile, only if the user_all_threads is not
    # empty (otherwise it will give an error since the result becomes an empty data frame
    # without any columns)
    if args.num_posts_quantile_threshold > 0:
      if not user_all_threads.empty:
        user_all_threads = user_all_threads.groupby('thread_id').apply(
            lambda x: x[x['num_posts'] >=
                        x['num_posts'].quantile(args.num_posts_quantile_threshold,
                                                interpolation='linear')])
        user_all_threads = user_all_threads.reset_index(1, drop=True)

    user_all_threads = user_all_threads.reset_index()
    user_all_threads_count = user_all_threads[['user', 'thread_id']].groupby('user').size()
    user_all_threads_stats = compute_discrete_stats(user_all_threads_count)
    
    
    # user stats only in all announcement threads: need a count of this coin users' posts in
    # all announcement threads
    all_announcement_thread_ids = [id for ids in announcement_thread_ids_by_symbol.values() for id in ids]
    user_all_announcement_posts = user_all_posts.loc[
        user_all_posts['thread_id'].isin(all_announcement_thread_ids)]
    user_all_announcement_posts_count = user_all_announcement_posts.groupby('user').size()
    user_all_announcement_posts_stats = compute_discrete_stats(user_all_announcement_posts_count)


    # user stats in terms of participation in only announcement threads. Need number of
    # announcement threads the users have participated during the co-post period.
    #user_all_announcement_threads = user_all_announcement_posts[['user', 'thread_id']].drop_duplicates()
    user_all_announcement_threads = user_all_threads.loc[
        user_all_threads['thread_id'].isin(all_announcement_thread_ids)]
    user_all_announcement_threads_count = user_all_announcement_threads.groupby('user').size()
    user_all_announcement_threads_stats = compute_discrete_stats(user_all_announcement_threads_count)


  
    # thread stats: to how many unique subjects the announcement users have also
    # contributed? Need to create such a counter for all other threads. how strong is the
    # subject connected to all other subjects? how diverse?
    thread_all_threads_count = user_all_threads.groupby('thread_id').size()
    thread_all_threads_stats = compute_discrete_stats(thread_all_threads_count)

    # thread stats: just like above, but how well this announcement thread is connected to
    # other (only) annoucement threads
    thread_all_announcement_threads_count = user_all_announcement_threads.groupby('thread_id').size()
    thread_all_announcement_threads_stats = compute_discrete_stats(thread_all_announcement_threads_count)

    concur = '_concurrent' if concurrent else ''
    # the first element is num_unique users which is already included in
    # user_announcement_posts_stats
    output['user_all_posts_mean{}'.format(concur)] = user_all_posts_stats['mean']
    output['user_all_posts_entropy{}'.format(concur)] = user_all_posts_stats['entropy']
    output['user_all_posts_entropy_normalized{}'.format(concur)] = user_all_posts_stats['entropy_normalized']
    
    output['bitcoin_forum_num_unique_users{}'.format(concur)] = bitcoin_forum_user_posts_stats['num_unique']
    output['bitcoin_forum_total_num_posts{}'.format(concur)] = bitcoin_forum_total_num_posts
    output['bitcoin_forum_posts_mean{}'.format(concur)] = bitcoin_forum_user_posts_stats['mean']
    output['bitcoin_forum_user_posts_mean{}'.format(concur)] = bitcoin_forum_user_posts_stats['mean']
    output['bitcoin_forum_user_posts_entropy{}'.format(concur)] = bitcoin_forum_user_posts_stats['entropy']
    output['bitcoin_forum_user_posts_entropy_normalized{}'.format(concur)] = bitcoin_forum_user_posts_stats['entropy_normalized']

    output['user_all_threads_mean{}'.format(concur)] = user_all_threads_stats['mean']
    output['user_all_threads_entropy{}'.format(concur)] = user_all_threads_stats['entropy']
    output['user_all_threads_entropy_normalized{}'.format(concur)] = user_all_threads_stats['entropy_normalized']
    
    output['user_all_announcement_posts_mean{}'.format(concur)] = user_all_announcement_posts_stats['mean']
    output['user_all_announcement_posts_entropy{}'.format(concur)] = user_all_announcement_posts_stats['entropy']
    output['user_all_announcement_posts_entropy_normalized{}'.format(concur)] = user_all_announcement_posts_stats['entropy_normalized']
    
    output['user_all_announcement_threads_mean{}'.format(concur)] = user_all_announcement_threads_stats['mean']
    output['user_all_announcement_threads_entropy{}'.format(concur)] = user_all_announcement_threads_stats['entropy']
    output['user_all_announcement_threads_entropy_normalized{}'.format(concur)] = user_all_announcement_threads_stats['entropy_normalized']
    
    output['num_unique_threads{}'.format(concur)] = thread_all_threads_stats['num_unique']
    output['thread_all_threads_mean{}'.format(concur)] = thread_all_threads_stats['mean']
    output['thread_all_threads_entropy{}'.format(concur)] = thread_all_threads_stats['entropy']
    output['thread_all_threads_entropy_normalized{}'.format(concur)] = thread_all_threads_stats['entropy_normalized']
    
    output['num_unique_announcement_threads{}'.format(concur)] = thread_all_announcement_threads_stats['num_unique']
    output['thread_all_announcement_threads_mean{}'.format(concur)] = thread_all_announcement_threads_stats['mean']
    output['thread_all_announcement_threads_entropy{}'.format(concur)] = thread_all_announcement_threads_stats['entropy']
    output['thread_all_announcement_threads_entropy_normalized{}'.format(concur)] = thread_all_announcement_threads_stats['entropy_normalized']



def compute_coin_age_stats_by_period(posts, announcement_users, start_date, end_date,
                                     output):
  ''' Computes the coin stats derived from the age of announcement users on the
  announcement thread.
 
  posts: all posts made by all users in all threads.
  announcement_users: the users who have made a post in the announcement thread between
  start and end dates.
  '''
  
  # user age/seniority stats in the forum. age is a continuous variable, so its entropy
  # is computed differently. Need to find the first post of each user in the announcement
  # thread
  user_first_post_dates = posts.loc[(posts['user'].isin(announcement_users)) &
                                    (posts['date'] <= end_date),
                                    ['user', 'date']].groupby('user').min()
  user_ages = [(start_date - x).days for x in user_first_post_dates['date']]
  user_ages_stats = compute_continous_stats(user_ages)
    
  output['user_ages_mean'] = user_ages_stats['mean']
  output['user_ages_median'] = user_ages_stats['median']
  output['user_ages_std'] = user_ages_stats['std']
  output['user_ages_entropy'] = user_ages_stats['entropy']
  output['user_ages_discrete_entropy'] = user_ages_stats['discrete_entropy']



def compute_text_analysis_stats_by_period(announcement_posts, output):
  ''' Computes the stats derived from sentiment and responding pattern in the announcement
  thread of the coin.

  announcement_posts: the posts made in the announcement thread between start and end date
  '''

  # conversation stats in announcement posts. Need to count how many reply branches were
  # generated and how many replies in each branch exist.
  conversation_branches = get_conversation_branches(announcement_posts)
  num_converstaion_branches = len(conversation_branches)
  num_replies_total = len(announcement_posts[announcement_posts['num_quotes'] > 0].index)
  num_announcement_posts = len(announcement_posts.index)
  fraction_user_num_replies = (0 if num_announcement_posts == 0 else
                               (num_replies_total * 1.0 / num_announcement_posts))
  conversation_stats = compute_continous_stats(conversation_branches)

  
  # posts token stats: total number of tokens and number of sentimental tokens plus their
  # mean/median/std across all announcement posts
  total_tokens = announcement_posts['num_tokens'].sum()
  tokens_stats = compute_continous_stats(announcement_posts['num_tokens'])
  total_sentimental_tokens = announcement_posts['num_sentimental_tokens'].sum()
  sentimental_tokens_stats = compute_continous_stats(announcement_posts['num_sentimental_tokens'])


  # absolute sentiment score: total sum and mean/median/std across all announcement posts
  abs_scores = announcement_posts['dictionary_absolute_sentiment_score']
  total_absolute_sentiment_score = abs_scores.sum()
  absolute_sentiment_score_stats = compute_continous_stats(abs_scores)
  
  
  # relative sentiment score among all post tokens: total sum and mean/median/std across all
  # announcement posts
  rel_scores = announcement_posts['dictionary_relative_sentiment_score_all_tokens']
  total_relative_sentiment_score_all_tokens = rel_scores.sum()
  relative_sentiment_score_all_tokens_stats = compute_continous_stats(rel_scores)


  # relative sentiment score only among post sentimental tokens: total sum and
  # mean/median/std across all announcement posts
  dated_rel_scores = announcement_posts[['date', 'dictionary_relative_sentiment_score_sentimental_tokens']]
  rel_scores = dated_rel_scores['dictionary_relative_sentiment_score_sentimental_tokens']
  total_relative_sentiment_score_sentimental_tokens = rel_scores.sum()
  relative_sentiment_score_sentimental_tokens_stats = compute_continous_stats(rel_scores)
  relative_sentiment_score_sentimental_tokens_polarization_stats = compute_polarization_stats(rel_scores, -5, 5)
  
  
  output['fraction_user_num_replies'] = fraction_user_num_replies
  output['num_conversations'] = num_converstaion_branches
  output['num_replies_total'] = num_replies_total
  output['converstaion_replies_mean'] = conversation_stats['mean']
  output['converstaion_replies_median'] = conversation_stats['median']
  output['converstaion_replies_std'] = conversation_stats['std']
  output['converstaion_replies_max'] = conversation_stats['max']
  
  output['total_tokens'] = total_tokens
  output['announcement_posts_tokens_mean'] = tokens_stats['mean']
  output['announcement_posts_tokens_median'] = tokens_stats['median']
  output['announcement_posts_tokens_std'] = tokens_stats['std']
  
  output['total_sentimental_tokens'] = total_sentimental_tokens
  output['announcement_posts_sentimental_tokens_mean'] = sentimental_tokens_stats['mean']
  output['announcement_posts_sentimental_tokens_median'] = sentimental_tokens_stats['median']
  output['announcement_posts_sentimental_tokens_std'] = sentimental_tokens_stats['std']
  
  output['total_abs_sentiment_score'] = total_absolute_sentiment_score
  output['abs_sentiment_score_mean'] = absolute_sentiment_score_stats['mean']
  output['abs_sentiment_score_median'] = absolute_sentiment_score_stats['median']
  output['abs_sentiment_score_std'] = absolute_sentiment_score_stats['std']
  
  output['total_rel_sentiment_score_all_tokens'] = total_relative_sentiment_score_all_tokens
  output['rel_sentiment_score_mean_all_tokens'] = relative_sentiment_score_all_tokens_stats['mean']
  output['rel_sentiment_score_median_all_tokens'] = relative_sentiment_score_all_tokens_stats['median']
  output['rel_sentiment_score_std_all_tokens'] = relative_sentiment_score_all_tokens_stats['std']
  
  output['total_rel_sentiment_score_sentimental_tokens'] = total_relative_sentiment_score_sentimental_tokens
  output['rel_sentiment_score_mean_sentimental_tokens'] = relative_sentiment_score_sentimental_tokens_stats['mean']
  output['rel_sentiment_score_median_sentimental_tokens'] = relative_sentiment_score_sentimental_tokens_stats['median']
  output['rel_sentiment_score_std_sentimental_tokens'] = relative_sentiment_score_sentimental_tokens_stats['std']
  
  output['normalized_rel_sentiment_mean'] = relative_sentiment_score_sentimental_tokens_polarization_stats['mean']
  output['normalized_rel_sentiment_skewness'] = relative_sentiment_score_sentimental_tokens_polarization_stats['skewness']
  output['normalized_rel_sentiment_kurtosis'] = relative_sentiment_score_sentimental_tokens_polarization_stats['kurtosis']
  output['normalized_rel_sentiment_entropy_normalized'] = relative_sentiment_score_sentimental_tokens_polarization_stats['entropy_normalized']
  output['normalized_rel_sentiment_pairwise_diffs_mean'] = relative_sentiment_score_sentimental_tokens_polarization_stats['pairwise_diffs_mean']
  output['normalized_rel_sentiment_gravity_centers_distance'] = relative_sentiment_score_sentimental_tokens_polarization_stats['gravity_centers_distance']
  output['normalized_rel_sentiment_negative_positive_size_diff'] = relative_sentiment_score_sentimental_tokens_polarization_stats['negative_positive_size_diff']
  output['normalized_rel_sentiment_morales_polarization'] = relative_sentiment_score_sentimental_tokens_polarization_stats['morales_polarization']


def compute_coin_stats_by_period(coin_symbol, announcement_thread_ids_by_symbol,
                                 posts, start_date, end_date):
  '''
  announcement_thread_ids_by_symbol: a dict from coin symbol to list of its discussion
  thread ids
  '''
  output = dict()
  announcement_thread_ids = announcement_thread_ids_by_symbol[coin_symbol]
  # get all posts during the start-end period in the announcement thread
  announcement_posts = posts.loc[(posts['thread_id'].isin(announcement_thread_ids)) &
                                 (posts['date'] <= end_date) &
                                 (posts['date'] >= start_date)]

  
  # find the posters sorted by their first posts.
  sorted_users = announcement_posts.groupby('user')['date_time'].min().sort_values()

  # user stats only in the announcement thread
  user_announcement_posts_count = announcement_posts.groupby('user').size()
  for q in POSTS_QUANTILES:
    output['user_num_posts_quantile_{}'.format(q*100)] = user_announcement_posts_count.quantile(q)
  
  user_announcement_posts_count = user_announcement_posts_count[
      user_announcement_posts_count >= args.num_posts_absolute_threshold]
  # find the minimum number of posts corresponding to the quantile threshold and remove
  # users who have less than that many posts
  if args.num_posts_quantile_threshold > 0:
    min_num_posts = user_announcement_posts_count.quantile(args.num_posts_quantile_threshold,
                                                           interpolation='linear')
    user_announcement_posts_count = user_announcement_posts_count[
        user_announcement_posts_count >= min_num_posts]

  num_posts_between_start_end_date = user_announcement_posts_count.sum()
  user_announcement_posts_stats = compute_discrete_stats(user_announcement_posts_count)

  # compute the stats for first 1,2,5,10,25,50 posters
  for num_first_posters in [1,2,5,10,25,50]:
    users = set()
    for user, date in sorted_users.iteritems():
      if user in user_announcement_posts_count.index:
        users.add(user)
      if len(users) >= num_first_posters:
        break
    # users now has the first n posters
    first_posters = user_announcement_posts_count.loc[users]
    rest_posters = user_announcement_posts_count.loc[~user_announcement_posts_count.index.isin(users)]
    first_posters_fraction = (first_posters.sum() * 1.0/ num_posts_between_start_end_date
                              if len(first_posters) > 0 else 0)
    rest_posters_fraction = (rest_posters.sum() * 1.0/ num_posts_between_start_end_date
                             if len(rest_posters) > 0 else 0)
    first_posters_stats = compute_discrete_stats(first_posters)
    rest_posters_stats = compute_discrete_stats(rest_posters)
    output['first_{}_posters_fraction_posts'.format(num_first_posters)] = first_posters_fraction
    output['first_{}_posters_posts_mean'.format(num_first_posters)] = first_posters_stats['mean']
    output['first_{}_posters_posts_entropy_normalized'.format(num_first_posters)] = first_posters_stats['entropy_normalized']
    output['rest_{}_posters_fraction_posts'.format(num_first_posters)] = rest_posters_fraction
    output['rest_{}_posters_posts_mean'.format(num_first_posters)] = rest_posters_stats['mean']
    output['rest_{}_posters_posts_entropy_normalized'.format(num_first_posters)] = rest_posters_stats['entropy_normalized']

  # user stats in all threads, not just the announcement thread: first get the list of
  # users who posted in the thread, then extract all their posts in all threads during the
  # start-end period (concurrent) or before end date (not concurrent), finally compute how
  # many posts each user has made in all threads
  #announcement_users = announcement_posts['user'].unique()
  announcement_users = user_announcement_posts_count.index.values
  
  output['announcement_thread_ids'] = ','.join(announcement_thread_ids)
  output['user_num_posts'] = num_posts_between_start_end_date
  
  output['num_unique_users'] = user_announcement_posts_stats['num_unique']
  output['user_announcement_posts_mean'] = user_announcement_posts_stats['mean']
  output['user_announcement_posts_entropy'] = user_announcement_posts_stats['entropy']
  output['user_announcement_posts_entropy_normalized'] = user_announcement_posts_stats['entropy_normalized']
  

  compute_coin_coposts_stats_by_period(announcement_thread_ids_by_symbol, posts,
                                       announcement_users, start_date, end_date, output)
  compute_coin_age_stats_by_period(posts, announcement_users, start_date, end_date, output)
  compute_text_analysis_stats_by_period(announcement_posts, output)
  
  return output


def compute_coins_stats(announcement_thread_ids_by_symbol, coin_data_by_symbol, posts):
  stats = list()
  for coin_symbol in announcement_thread_ids_by_symbol.keys():
    if args.end_date:
      end_date = datetime.strptime(args.end_date, "%Y-%m-%d")
      start_date = end_date - timedelta(days=args.period_length)
    else:
      # earliest trade date as end date
      end_date = coin_data_by_symbol.loc[coin_symbol, 'earliest_trade_date']
      end_date += timedelta(days=args.earliest_trade_date_shift)
      start_date = end_date - timedelta(days=args.period_length)

    output = compute_coin_stats_by_period(coin_symbol, announcement_thread_ids_by_symbol,
                                          posts, start_date, end_date)
    # Now fill in the output row with common info
    coin_data = coin_data_by_symbol.loc[coin_symbol, ]
    output['symbol'] = coin_symbol
    for col, val in coin_data.iteritems():
      if 'date' not in col:
        output[col] = val
      else:
        # convert datetimes to date in the output csv
        output[col] = val.date() if val else ''
    stats.append(output)

  return stats


if __name__ == '__main__':
  if os.path.isfile(args.output_dir):
    sys.exit("Output dir " + args.output_dir + " is an existing file.")
  if not os.path.isdir(args.output_dir):
    os.makedirs(args.output_dir)

  posts = utils.read_forum_data(args.forum_input)
  # read coins data and index by coin symbol
  coin_data_by_symbol = pd.read_csv(args.coins_info, header = 0, index_col = 0,
                                    parse_dates= ['earliest_trade_date', 'user1_date',
                                                  'user2_date', 'user3_date', 'user4_date'])
  coin_data_by_symbol = coin_data_by_symbol.fillna('')

  # Limit the coins and their announcement thread ids only to the coins that are provided
  # in the valid_coin_symbols_file. This will only compute metrics for such coins and
  # those metrics that rely on their connection to other announcement threads will change
  # too, as their connection only with this limited announcement threads will be measured
  valid_coin_symbols = list(coin_data_by_symbol.index)
  if args.valid_coin_symbols:
    with open(args.valid_coin_symbols) as f:
      valid_coins = f.readlines()
    valid_coin_symbols = list(set(valid_coins))
  coin_data_by_symbol = coin_data_by_symbol.loc[valid_coin_symbols]

  # mapping from coin symbol to announcement thread ids
  announcement_thread_ids_by_symbol = defaultdict(list)
  url_columns = [col for col in coin_data_by_symbol.columns if 'url' in col]
  for coin_symbol, row in coin_data_by_symbol.iterrows():
    for announcement_url in row[url_columns]:
      if not announcement_url:
        continue
      url_posts = posts.loc[posts['url'] == announcement_url, ['url', 'thread_id']]
      if url_posts.empty:
        print 'Coin {} announcement url {} not in the forums.'.format(coin_symbol, announcement_url)
        continue

      thread_ids = url_posts['thread_id'].unique()
      assert len(thread_ids) == 1, 'URL {} has multiple thread ids {}'.format(announcement_url, thread_ids)
      announcement_thread_ids_by_symbol[coin_symbol].append(thread_ids[0])


  coin_data_columns = list(coin_data_by_symbol.columns.values)
  # all stats are up to trade date!
  fields = (['symbol'] + coin_data_columns)
  fields.extend(['announcement_thread_ids', 'user_num_posts', 'fraction_user_num_replies'])
  fields.extend(['num_unique_users', 'user_announcement_posts_mean',
                 'user_announcement_posts_entropy',
                 'user_announcement_posts_entropy_normalized'])
  fields.extend(['user_num_posts_quantile_{}'.format(q*100) for q in POSTS_QUANTILES])

  for concur in ['', '_concurrent']:
    fields.extend(['bitcoin_forum_num_unique_users{}'.format(concur),
                   'bitcoin_forum_total_num_posts{}'.format(concur),
                   'bitcoin_forum_posts_mean{}'.format(concur),
                   'bitcoin_forum_user_posts_mean{}'.format(concur),
                   'bitcoin_forum_user_posts_entropy{}'.format(concur),
                   'bitcoin_forum_user_posts_entropy_normalized{}'.format(concur)])
    fields.extend(['user_all_posts_mean{}'.format(concur),
                   'user_all_posts_entropy{}'.format(concur),
                   'user_all_posts_entropy_normalized{}'.format(concur)])
    fields.extend(['user_all_threads_mean{}'.format(concur),
                   'user_all_threads_entropy{}'.format(concur),
                   'user_all_threads_entropy_normalized{}'.format(concur)])
    fields.extend(['user_all_announcement_posts_mean{}'.format(concur),
                   'user_all_announcement_posts_entropy{}'.format(concur),
                   'user_all_announcement_posts_entropy_normalized{}'.format(concur)])
    fields.extend(['user_all_announcement_threads_mean{}'.format(concur),
                   'user_all_announcement_threads_entropy{}'.format(concur),
                   'user_all_announcement_threads_entropy_normalized{}'.format(concur)])
    fields.extend(['num_unique_threads{}'.format(concur),
                   'thread_all_threads_mean{}'.format(concur),
                   'thread_all_threads_entropy{}'.format(concur),
                   'thread_all_threads_entropy_normalized{}'.format(concur)])
    fields.extend(['num_unique_announcement_threads{}'.format(concur),
                   'thread_all_announcement_threads_mean{}'.format(concur),
                   'thread_all_announcement_threads_entropy{}'.format(concur),
                   'thread_all_announcement_threads_entropy_normalized{}'.format(concur)])
  for num_first_posters in [1,2,5,10,25,50]:
    fields.extend(['first_{}_posters_fraction_posts'.format(num_first_posters),
                   'first_{}_posters_posts_mean'.format(num_first_posters),
                   'first_{}_posters_posts_entropy_normalized'.format(num_first_posters),
                   'rest_{}_posters_fraction_posts'.format(num_first_posters),
                   'rest_{}_posters_posts_mean'.format(num_first_posters),
                   'rest_{}_posters_posts_entropy_normalized'.format(num_first_posters)])

  fields.extend(['user_ages_mean', 'user_ages_median',
                 'user_ages_std', 'user_ages_entropy', 'user_ages_discrete_entropy'])
  fields.extend(['num_conversations',
                 'num_replies_total',
                 'converstaion_replies_mean',
                 'converstaion_replies_median',
                 'converstaion_replies_std',
                 'converstaion_replies_max'])
  fields.extend(['total_tokens',
                 'announcement_posts_tokens_mean',
                 'announcement_posts_tokens_median',
                 'announcement_posts_tokens_std'])
  fields.extend(['total_sentimental_tokens',
                 'announcement_posts_sentimental_tokens_mean',
                 'announcement_posts_sentimental_tokens_median',
                 'announcement_posts_sentimental_tokens_std'])
  fields.extend(['total_abs_sentiment_score',
                 'abs_sentiment_score_mean',
                 'abs_sentiment_score_median',
                 'abs_sentiment_score_std'])
  fields.extend(['total_rel_sentiment_score_all_tokens',
                 'rel_sentiment_score_mean_all_tokens',
                 'rel_sentiment_score_median_all_tokens',
                 'rel_sentiment_score_std_all_tokens'])
  fields.extend(['total_rel_sentiment_score_sentimental_tokens',
                 'rel_sentiment_score_mean_sentimental_tokens',
                 'rel_sentiment_score_median_sentimental_tokens',
                 'rel_sentiment_score_std_sentimental_tokens'])
  fields.extend(['normalized_rel_sentiment_mean',
                 'normalized_rel_sentiment_skewness',
                 'normalized_rel_sentiment_kurtosis',
                 'normalized_rel_sentiment_entropy_normalized',
                 'normalized_rel_sentiment_pairwise_diffs_mean',
                 'normalized_rel_sentiment_gravity_centers_distance',
                 'normalized_rel_sentiment_negative_positive_size_diff',
                 'normalized_rel_sentiment_morales_polarization'])


  period_end = "enddate-" + (args.end_date if args.end_date else "earliesttrade")

  # generate networks
  directed_network_filename = os.path.join(args.output_dir,
      "directed_network_{}_l{}_ta{}_tq{}_s{}.csv".format(period_end,
                                                   args.period_length,
                                                   args.num_posts_absolute_threshold,
                                                   args.num_posts_quantile_threshold,
                                                   args.earliest_trade_date_shift))
  undirected_network_filename = os.path.join(args.output_dir,
      "undirected_network_{}_l{}_ta{}_tq{}_s{}.csv".format(period_end,
                                                   args.period_length,
                                                   args.num_posts_absolute_threshold,
                                                   args.num_posts_quantile_threshold,
                                                   args.earliest_trade_date_shift))
  (undirected_network, directed_network) = generate_network(
      announcement_thread_ids_by_symbol, posts, coin_data_by_symbol)
  undirected_network.to_csv(undirected_network_filename, index=False)
  directed_network.to_csv(directed_network_filename, index=False)
 
  # thread stats
  coin_threads_filename = os.path.join(args.output_dir,
                                      "coin_announcement_thread_stats_{}_l{}_ta{}_tq{}_s{}.csv"
                                       .format(period_end,
                                               args.period_length,
                                               args.num_posts_absolute_threshold,
                                               args.num_posts_quantile_threshold,
                                               args.earliest_trade_date_shift))
  stats = compute_coins_stats(announcement_thread_ids_by_symbol, coin_data_by_symbol, posts)
  with open(coin_threads_filename, 'w') as csvfile:
    csvwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fields)
    csvwriter.writeheader()
    for coin_stat in stats:
      csvwriter.writerow(coin_stat)
