#!/usr/bin/python

# code to analysis its results
#
#
#data = pickle.load(open('./users_post_count_per_symbol_enddate-2016-11-01_l300.pickle', 'rb'))
#stats = pd.read_csv('../final_prices_initial_100days_threads_initial_2000days.csv')
#cstats = stats[['symbol', 'user_announcement_posts_entropy_normalized', 'price_mean_volatility_fixed_100days']]
#cstats = cstats.set_index('symbol')
#for coin, posts in data.iteritems():
#  num_posts = np.sum(posts)
#  if num_posts > 200 and coin in cstats.index:
#    ent = cstats.loc[coin, 'user_announcement_posts_entropy_normalized']
#    if ent > 0.85:
#      vol = np.log(cstats.loc[coin, 'price_mean_volatility_fixed_100days'])
#      print('{}: num posts {}  ent {} vol {}'.format(coin, num_posts, ent, vol))
#
#data = pickle.load(open('./users_post_count_per_symbol_enddate-earliest_trade_l2000.pickle', 'rb'))
#stats = pd.read_csv('../final_prices_initial_100days_threads_initial_2000days.csv')
#cstats = stats[['symbol', 'user_announcement_posts_entropy_normalized', 'price_mean_volatility_initial_100days', 'user_num_posts', 'user1_url', 'user2_url']]
#cstats = cstats.set_index('symbol')
#print cstats
#for coin, posts in data.iteritems():
#  num_posts = np.sum(posts)
#  if num_posts > 500 and coin in cstats.index:
#    ent = cstats.loc[coin, 'user_announcement_posts_entropy_normalized']
#    if ent < 0.75:
#      vol = np.log(cstats.loc[coin, 'price_mean_volatility_initial_100days'])
#      num_posts = cstats.loc[coin, 'user_num_posts']
#      url1 = cstats.loc[coin, 'user1_url']
#      url2 = cstats.loc[coin, 'user2_url']
#      print('{}: num posts {}  ent {} vol {} urls {} {}'.format(coin, num_posts, ent, vol, url1, url2))



import argparse
import os
import sys
import csv
import pickle
import pandas as pd
import utils
from datetime import timedelta, datetime
from collections import defaultdict

parser = argparse.ArgumentParser(
    description="This script generates some stats on the whole user activity on the "
                "announcement threads. In particular, it computes how many unique users "
                "have been present in the forums during periods of different lenghts.")
parser.add_argument("forum_input",
                    help="The location of CSV file containing posts from all forums "
                    "sorted by date and time. The last two columns of file should "
                    "contain the list of modified and unmodified coins mentions in the "
                    "post.")
parser.add_argument("coins_info",
                    help="A CSV file containing the earliest introduction and trade "
                    "dates for each coin. It also has the urls of different announcement "
                    "threads. The first line is header with info like: "
                    "symbol,name,earliest_trade_date,,user1,user1_url,user1_date"
                    "user2,user2_url,user2_date... This file is generated by "
                    "extract_coin_users.py.")
parser.add_argument("output_dir",
                    help="The directory where the subjects stats will be written to. "
                    "There will be two files, one containing all subjects with "
                    "subject_id in the first column and its url in the second column. "
                    "The second output will contain coin, subject_id, url, "
                    "introduction_date and trade date as first few columns.")
parser.add_argument("-t", dest="num_posts_threshold", type=int, default=0,
                    help="Defines the threshold for the minimum number of posts a user "
                    "should have made in a thread before it's considered the user has "
                    "actually contributed to the thread. Default is 0.")
parser.add_argument("-v", dest="valid_coin_symbols",
                    help="If provided, it has to be a file with the symbol of valid "
                    "coins, one per line. If provided, the metrics only for these "
                    "coins will be computed. Also all the metrics that relate the coin "
                    "to other announcement threads will be measured based on the coins "
                    "present in this set and not all the coins present in coins_info "
                    "For example, num_unique_announcement_threads will contain the "
                    "number announcement threads belonging to these limited coins that "
                    "are connected to the coin of interest.")
args = parser.parse_args()


def users_per_symbol(announcement_thread_ids_by_symbol, posts, earliest_trade_dates):
  for end_date_str in ['2016-11-01']:
    for period_num_days in [600, 400, 300, 200, 100]:
      users_by_symbol = dict()
      for symbol, announcement_thread_ids in announcement_thread_ids_by_symbol.iteritems():
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        start_date = end_date - timedelta(days=period_num_days)

        # get all posts during the start-end period in the announcement thread
        announcement_posts = posts.loc[(posts['thread_id'].isin(announcement_thread_ids)) &
                                       (posts['date'] <= end_date) &
                                       (posts['date'] >= start_date)]
        posts_count_per_user = announcement_posts.groupby('user').size()
        # Filter to only those users who have made enough posts
        posts_count_per_user = posts_count_per_user[posts_count_per_user > args.num_posts_threshold]
        posts_count_per_user = posts_count_per_user.rename('posts_count')
        users_by_symbol[symbol] = posts_count_per_user

      output_filename = os.path.join(
          args.output_dir, "users_post_count_per_symbol_enddate-{}_l{}.pickle".format(end_date_str, period_num_days))
      with open(output_filename, "wb") as f:
        pickle.dump(users_by_symbol, f)
   
  # Now, do the same for period immediately beofore trading
  period_num_days = 2000  
  users_by_symbol = dict()
  for symbol, announcement_thread_ids in announcement_thread_ids_by_symbol.iteritems():
    end_date = earliest_trade_dates[symbol]
    start_date = end_date - timedelta(days=period_num_days)
    # get all posts during the start-end period in the announcement thread
    announcement_posts = posts.loc[(posts['thread_id'].isin(announcement_thread_ids)) &
                                   (posts['date'] <= end_date) &
                                   (posts['date'] >= start_date)]
    posts_count_per_user = announcement_posts.groupby('user').size()
    # Filter to only those users who have made enough posts
    posts_count_per_user = posts_count_per_user[posts_count_per_user > args.num_posts_threshold]
    posts_count_per_user = posts_count_per_user.rename('posts_count')
    users_by_symbol[symbol] = posts_count_per_user

  output_filename = os.path.join(
      args.output_dir, "users_post_count_per_symbol_enddate-earliest_trade_l{}.pickle".format(period_num_days))
  with open(output_filename, "wb") as f:
    pickle.dump(users_by_symbol, f)


def compute_user_analysis(announcement_thread_ids_by_symbol, posts):
  stats = list()
  announcement_thread_ids = [t_id for coin_t_ids in announcement_thread_ids_by_symbol.values()
                                  for t_id in coin_t_ids]
  for end_date_str in ['2016-12-15', '2016-12-01', '2016-11-15', '2016-11-01',
                       '2016-10-15', '2016-10-01', '2016-09-15', '2016-09-01',
                       '2016-08-15', '2016-08-01', '2016-07-15', '2016-07-01',
                       '2016-06-15', '2016-06-01', '2016-05-15', '2016-05-01',
                       '2016-04-15', '2016-04-01', '2016-04-15', '2016-04-01',
                       '2016-03-15', '2016-03-01', '2016-02-15', '2016-02-01']:
    for period_num_days in [600, 400, 300, 200, 100]:
      end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
      start_date = end_date - timedelta(days=period_num_days)

      # get all posts during the start-end period in the announcement thread
      announcement_posts = posts.loc[(posts['thread_id'].isin(announcement_thread_ids)) &
                                     (posts['date'] <= end_date) &
                                     (posts['date'] >= start_date)]
      posts_count_per_user = announcement_posts.groupby('user').size()
      # Filter to only those users who have made enough posts
      posts_count_per_user = posts_count_per_user[posts_count_per_user > args.num_posts_threshold]
      num_users = posts_count_per_user.size
      stats.append({'end_date': end_date.strftime('%Y-%m-%d'),
                    'start_date': start_date.strftime('%Y-%m-%d'),
                    'period_num_days': period_num_days,
                    'total_num_user': num_users})

  return stats


if __name__ == '__main__':
  posts = utils.read_forum_data(args.forum_input)
  # read coins data and index by coin symbol
  coin_data_by_symbol = pd.read_csv(args.coins_info, header = 0, index_col = 0,
                                    parse_dates= ['earliest_trade_date', 'user1_date',
                                                  'user2_date', 'user3_date', 'user4_date'])
  coin_data_by_symbol = coin_data_by_symbol.fillna('')
  earliest_trade_dates = coin_data_by_symbol['earliest_trade_date'].to_dict()

  # Limit the coins and their announcement thread ids only to the coins that are provided
  # in the valid_coin_symbols_file. This will only compute metrics for such coins and
  # those metrics that rely on their connection to other announcement threads will change
  # too, as their connection only with this limited announcement threads will be measured
  valid_coin_symbols = list(coin_data_by_symbol.index)
  if args.valid_coin_symbols:
    with open(args.valid_coin_symbols) as f:
      valid_coins = f.read().splitlines()
    valid_coin_symbols = list(set(valid_coins))
  coin_data_by_symbol = coin_data_by_symbol.loc[valid_coin_symbols]

  # mapping from coin symbol to announcement thread ids
  announcement_thread_ids_by_symbol = defaultdict(list)
  url_columns = [col for col in coin_data_by_symbol.columns if 'url' in col]
  for coin_symbol, row in coin_data_by_symbol.iterrows():
    for announcement_url in row[url_columns]:
      if not announcement_url:
        continue
      url_posts = posts.loc[posts['url'] == announcement_url, ['url', 'thread_id']]
      if url_posts.empty:
        print 'Coin {} announcement url {} not in the forums.'.format(coin_symbol, announcement_url)
        continue

      thread_ids = url_posts['thread_id'].unique()
      assert len(thread_ids) == 1, 'URL {} has multiple thread ids {}'.format(announcement_url, thread_ids)
      announcement_thread_ids_by_symbol[coin_symbol].append(thread_ids[0])


  users_per_symbol(announcement_thread_ids_by_symbol, posts, earliest_trade_dates)
  sys.exit(1)
  fields = ['end_date', 'start_date', 'period_num_days', 'total_num_user']
  
  output_filename = os.path.join(args.output_dir,
                                 "user_analysis_by_period_t{}.csv"
                                 .format(args.num_posts_threshold))
  stats = compute_user_analysis(announcement_thread_ids_by_symbol, posts)
  with open(output_filename, 'w') as csvfile:
    csvwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fields)
    csvwriter.writeheader()
    for stat in stats:
      csvwriter.writerow(stat)
